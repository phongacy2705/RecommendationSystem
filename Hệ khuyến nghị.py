# -*- coding: utf-8 -*-
"""Source.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17C5F3kXE45zqQFj4IjttAUvlER3ZfHb6

#Import các thư viện cần thiết
"""

!pip install pyvi

import pandas as pd
import numpy as np
import re
from pyvi import ViTokenizer

from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.metrics.pairwise import cosine_similarity,linear_kernel

from gensim.models import KeyedVectors
from gensim import models

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

from matplotlib import pyplot as plt

from sklearn.metrics import precision_score
from sklearn.metrics import recall_score

from collections import defaultdict

"""#Đọc dữ liệu"""

dataset = pd.read_csv('/content/drive/MyDrive/DS300.M11/dataset/vnexpress_news.csv')

dataset['UserID']

Title_length = []
for title in dataset['NewsTitle']:
  length = ViTokenizer.tokenize(title)
  length = len(length.split())
  Title_length.append(length)

Abstract_length = []
for abs in dataset['NewsAbstract']:
  length = ViTokenizer.tokenize(abs)
  length = len(length.split())
  Abstract_length.append(length)

plt.hist(Title_length)

plt.hist(Abstract_length)

"""#Tiền xử lý dữ liệu"""

def get_stopwords_list(stop_file_path):
  with open(stop_file_path,'r',encoding="utf-8") as f:
    stopwords = f.readlines()
    stop_set = set(m.strip() for m in stopwords)
    return list(frozenset(stop_set))
stopwords = get_stopwords_list("/content/drive/MyDrive/DS300.M11/project/vietnamese-stopwords.txt")

def pre_processing(text):
  text = text.lower()
  # xóa \xa0
  text = re.sub('\xa0',' ',text)
  # Xóa dấu chấm, phẩy, hỏi ở cuối câu
  text = re.sub(r"[\.,\?]+$-", " ", text)
  # Xóa tất cả dấu trong câu
  text = text.replace(",", " ").replace(".", " ") \
        .replace(";", " ").replace("“", " ") \
        .replace(":", " ").replace("”", " ") \
        .replace('"', " ").replace("'", " ") \
        .replace("!", " ").replace("?", " ") \
        .replace("-", " ").replace("?", " ") \
        .replace("("," ").replace(")"," ") \
        .replace("%"," ").replace("+"," ") \
        .replace("="," ").replace("__"," ")
  text = text.strip()
  #tách từ
  text = ViTokenizer.tokenize(text)
  text = text.split()
  #xóa stopwords
  text = [w for w in text if w not in stopwords]
  text = " ".join(text)
  return text

"""#TfidfVectorizer"""

contents = dataset[['NewsTitle','NewsAbstract']].agg(' '.join, axis=1)
#contents = dataset['NewsAbstract']
#contents = dataset['NewsTitle']

contents

topic = np.array(pd.get_dummies(dataset['Topic']))

corpus = contents.apply(pre_processing)

vectorizer = TfidfVectorizer()

topic.shape

overview_matrix = vectorizer.fit_transform(corpus).toarray()
overview_matrix_tfidf = np.hstack((overview_matrix,topic))

cosine_similarity_matrix = cosine_similarity(overview_matrix_tfidf)

mapping = pd.Series(dataset.index,index=dataset['NewsTitle'])
mapping

def get_newtitle(newsid):
  return dataset[dataset['NewsID']==newsid]['NewsTitle'].values[0]

def get_newid(newtitle):
  return dataset[dataset['NewsTitle']==newtitle]['NewsID'].values[0]

"""#word2vec"""

word2vec_path = "/content/drive/MyDrive/DS300.M11/project/embedding/baomoi.window2.vn.model.bin.gz"
model_w2v = KeyedVectors.load_word2vec_format(word2vec_path,binary=True)

vocab = model_w2v.wv.vocab

max_len = 0
for sent in corpus:
  tok = sent.split()
  if(len(tok)>max_len):
    max_len = len(tok)
print(max_len)

tokenizer=Tokenizer()
tokenizer.fit_on_texts(corpus)
tokenized_documents=tokenizer.texts_to_sequences(corpus)
tokenized_paded_documents=pad_sequences(tokenized_documents,maxlen=max_len,padding='post')
vocab_size=len(tokenizer.word_index)+1
print (tokenized_paded_documents[0])

embedding_matrix=np.zeros((vocab_size,300))
for word,i in tokenizer.word_index.items():
    if word in model_w2v:
        embedding_matrix[i]=model_w2v[word]

document_word_embeddings=np.zeros((len(tokenized_paded_documents),max_len,300))
for i in range(len(tokenized_paded_documents)):
    for j in range(len(tokenized_paded_documents[0])):
        document_word_embeddings[i][j]=embedding_matrix[tokenized_paded_documents[i][j]]
document_word_embeddings.shape

document_embeddings=np.zeros((len(tokenized_paded_documents),300))
words=vectorizer.get_feature_names()
for i in range(len(document_word_embeddings)):
  for j in range(len(words)):
    if words[j] in tokenizer.word_index.keys():
      document_embeddings[i]+=embedding_matrix[tokenizer.word_index[words[j]]]*overview_matrix[i][j]
print(document_embeddings.shape)

overview_w2v = cosine_similarity(document_embeddings)

np.save('/content/drive/MyDrive/DS300.M11/project/w2v_similarity.npy',overview_w2v)

w2v_similarity = load('/content/drive/MyDrive/DS300.M11/project/w2v_similarity.npy')

w2v_similarity

"""#FastText"""

path = '/content/drive/MyDrive/DS300.M11/project/fasttext_similarity.npy'
from numpy import load
fasttext_similarity = load(path)
fasttext_similarity

fasttext_similarity.shape

"""# Jaccard"""

def jaccard_set(list1, list2):
    intersection = len(list(set(list1).intersection(list2)))
    union = (len(list1) + len(list2)) - intersection
    return float(intersection) / union

"""#Demo"""

def recommend_news(userid,newid,method ='tfidf'):
  title = get_newtitle(str(newid))
  new_index = mapping[title]
  print('Tiêu đề bài báo : ',title)
  print('')
  if method == 'tfidf':
    similarity_score = list(enumerate(cosine_similarity_matrix[new_index]))
  if method =='jaccard':
    jaccard_similarity = np.zeros(shape=(len(dataset)))
    for i in range(len(dataset)):
      jaccard_similarity[i] = jaccard_set(list(tokenized_documents[new_index]),list(tokenized_documents[i]))
    similarity_score = list(enumerate(jaccard_similarity))
  if method =='word2vec':
    similarity_score = list(enumerate(overview_w2v[new_index]))
  if method =='fasttext':
    similarity_score = list(enumerate(fasttext_similarity[new_index]))
  similarity_score = sorted(similarity_score, key = lambda x:x[1],reverse=True)
  similarity_score = similarity_score[1:10]
  recommendations = [(dataset['NewsTitle'].loc[index],score) for index,score in similarity_score]
  print('Danh sách top 10 các bài báo khuyến nghị dựa trên ' +str(method)+' cho user có id là {} : '.format(userid))
  print('')
  return recommendations

recommend_news(userid =1002611542 ,newid = 4404506,method = 'word2vec')

"""#Đánh giá"""

test = pd.read_csv('/content/drive/MyDrive/DS300.M11/dataset/test_set.csv')

def precision_recall(userid,threshold=0.5,method = 'tfidf'):
    """Return precision and recall at k metrics for each user"""
    id_true = test[test['UserID']==userid]['NewsID'].values[0].split()
    precision = 0
    recall = 0
    len_news = len(test[test['UserID']==userid]['NewsID'].values[0].split())/4
    for newid in test[test['UserID']==userid]['NewsID'].values[0].split():
      id_pred  = []
      for title,score in recommend_news(userid,newid,method):
        if score >=threshold:
          id_pred.append(get_newid(title))
      n_rel_and_rec_k = len(list(set(id_true).intersection(id_pred)))
      precision += n_rel_and_rec_k/len(id_pred) if id_pred!=[] else 0
      recall += n_rel_and_rec_k/len(id_true) if id_true!=[] else 0
    return precision/len_news,recall/len_news

'''for method in ['tfidf','word2vec','fasttext']:
  precision = [0 for i in range(len(test))]
  recall = [0 for i in range(len(test))]
  for i in range(len(test)):
    userid = test['UserID'].loc[i]
    precision[i], recall[i] = precision_recall(userid,method = method)
  print('Precision score of {}'.format(method),np.round(np.mean(precision)*100,2))
  print('Recall score of {}'.format(method),np.round(np.mean(recall)*100,2))'''